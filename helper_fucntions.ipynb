{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from scipy.stats import kurtosis, skew, pearsonr\n",
    "from numpy import nan\n",
    "from scipy.spatial import distance\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stat_per_hemis(strm_median_P,strm_median_H):\n",
    "    # stats for pataloghical hemispheres\n",
    "    tmp1 = np.mean(strm_median_P, axis=1)\n",
    "    tmp2 = np.std(strm_median_P, axis=1)\n",
    "    tmp3 = kurtosis(strm_median_P, axis = 1, fisher = True, bias = True, nan_policy = 'propagate')\n",
    "    tmp4 = skew(strm_median_P, axis = 1, bias=True)\n",
    "    # stats for pataloghical hemispheres\n",
    "    tmp11 = np.mean(strm_median_H, axis=1)\n",
    "    tmp22 = np.std(strm_median_H, axis=1)\n",
    "    tmp33 = kurtosis(strm_median_H, axis = 1, fisher = True, bias = True, nan_policy = 'propagate')\n",
    "    tmp44 = skew(strm_median_H, axis = 1, bias=True)\n",
    "    # consider the normalized dMRI tract profile\n",
    "    # feature = np.vstack((np.abs((tmp1-tmp11)/(tmp1+tmp11)), np.abs((tmp2-tmp22)/(tmp2+tmp22)), np.abs((tmp3-tmp33)/(tmp3+tmp33)),np.abs((tmp4-tmp44)/(tmp4+tmp44)))).T\n",
    "    # consider the differences between ipsilesional and contalesional dMRI tract profile\n",
    "    feature = np.vstack((tmp1-tmp11,tmp2-tmp22,tmp3-tmp33,tmp4-tmp44)).T\n",
    "    # consider both ipsilesional and contalesional dMRI tract profile \n",
    "    #feature = np.vstack((tmp1,tmp11,tmp2,tmp22,tmp3,tmp33,tmp4,tmp44)).T\n",
    "    return feature\n",
    "\n",
    "def extract_stat_dif_of_hemis(strm_median_P, strm_median_H):\n",
    "    \n",
    "    # consider the normalized dMRI tract profile\n",
    "    #feature_CST = (strm_median_P-strm_median_H)/(strm_median_P+strm_median_H)\n",
    "    # consider both ipsilesional and contalesional dMRI tract profile\n",
    "    feature_CST = (strm_median_P-strm_median_H)\n",
    "    tmp1 = np.mean(feature_CST, axis=1)\n",
    "    tmp2 = np.std(feature_CST, axis=1)\n",
    "    tmp3 = kurtosis(feature_CST, axis = 1, fisher = True, bias = True, nan_policy = 'propagate')\n",
    "    tmp4 = skew(feature_CST, axis = 1, bias=True)\n",
    "    feature = np.vstack((tmp1,tmp2,tmp3,tmp4)).T\n",
    "    \n",
    "    return feature\n",
    "   \n",
    "'''Try to correct pca values '''\n",
    "def pca_correction(pca, profile):  \n",
    "    r, p = pearsonr(pca, profile)\n",
    "    if r<0:\n",
    "        pca = -pca\n",
    "    return pca\n",
    "\n",
    "def mahalanobis(x=None, mu=None, inv_covmat=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - mu\n",
    "    # print('mean:',x_minus_mu.shape)\n",
    "    # if not cov:\n",
    "    #     cov = np.cov((data),rowvar=0)\n",
    "    #     #print('cov shape:', cov.shape)\n",
    "    # inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal\n",
    "def Clustering_streamlines(data):\n",
    "    # this function will weighing up the streamlines by computing the mahalanobis distance of each\n",
    "    # node with the mean one/ try to consider representative streamlines\n",
    "    # data shape = (5000,100,65)\n",
    "    #imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')#I can also change the strategy to mean    \n",
    "    D = np.zeros(shape=(data.shape[2],data.shape[0])) # D has shape = (65,5000)\n",
    "    for i in range(data.shape[2]):\n",
    "        #data[:,:,i] = imp_mean.fit_transform(data[:,:,i])\n",
    "        cov = np.cov((data[:,:,i]),rowvar=0)\n",
    "        inv_covmat = np.linalg.inv(cov)\n",
    "        mu = np.mean(data[:,:,i], axis=0)\n",
    "        for j in range(data.shape[0]):\n",
    "            D[i,j] = mahalanobis(data[j,:,i],mu,inv_covmat) \n",
    "            # cov = np.cov((data[:,:,i]),rowvar=0)\n",
    "            # inv_covmat = np.linalg.inv(cov)\n",
    "            # D[i,j] = distance.mahalanobis(data[j,:,i],np.mean(data[:,:,i], axis = 0),inv_covmat)\n",
    "        #print('subject=',i)    \n",
    "    return D\n",
    "\n",
    "def mahal_features(adc_data_P,fa_data_P,fd_data_P,rd_data_P,ad_data_P, adc_data_H,fa_data_H,fd_data_H,rd_data_H,ad_data_H):\n",
    "    \"\"\"\n",
    "        Try to create features of all streamline distances from mean profile \n",
    "    \"\"\"    \n",
    "    adc_mahalD_P = Clustering_streamlines(adc_data_P)\n",
    "    adc_mahalD_H = Clustering_streamlines(adc_data_H)\n",
    "    \n",
    "    fa_mahalD_P = Clustering_streamlines(fa_data_P)\n",
    "    fa_mahalD_H = Clustering_streamlines(fa_data_H)\n",
    "    \n",
    "    fd_mahalD_P = Clustering_streamlines(fd_data_P)\n",
    "    fd_mahalD_H = Clustering_streamlines(fd_data_H)\n",
    "       \n",
    "    adc_mahal_P = np.vstack((np.mean(adc_mahalD_P, axis=1),\n",
    "                              np.std(adc_mahalD_P, axis=1), \n",
    "                              kurtosis(adc_mahalD_P, axis=1, fisher=True, bias=True),\n",
    "                              skew(adc_mahalD_P, axis=1, bias=True)))\n",
    "    adc_mahal_H = np.vstack((np.mean(adc_mahalD_H, axis=1),\n",
    "                              np.std(adc_mahalD_H, axis=1), \n",
    "                              kurtosis(adc_mahalD_H, axis=1, fisher=True, bias=True),\n",
    "                              skew(adc_mahalD_H, axis=1, bias=True)))    \n",
    "    fa_mahal_P = np.vstack((np.mean(fa_mahalD_P,axis=1),\n",
    "                              np.std(fa_mahalD_P, axis=1), \n",
    "                              kurtosis(fa_mahalD_P, axis=1, fisher=True, bias=True),\n",
    "                              skew(fa_mahalD_P, axis=1, bias=True)))    \n",
    "    fa_mahal_H = np.vstack((np.mean(fa_mahalD_H, axis=1),\n",
    "                              np.std(fa_mahalD_H, axis=1), \n",
    "                              kurtosis(fa_mahalD_H, axis=1, fisher=True, bias=True),\n",
    "                              skew(fa_mahalD_H, axis=1, bias=True)))\n",
    "    fd_mahal_P = np.vstack((np.mean(fd_mahalD_P, axis=1),\n",
    "                              np.std(fd_mahalD_P, axis=1), \n",
    "                              kurtosis(fd_mahalD_P, axis=1, fisher=True, bias=True),\n",
    "                              skew(fd_mahalD_P, axis=1, bias=True)))\n",
    "    fd_mahal_H = np.vstack((np.mean(fd_mahalD_H, axis=1),\n",
    "                              np.std(fd_mahalD_H, axis=1), \n",
    "                              kurtosis(fd_mahalD_H, axis=1, fisher=True, bias=True),\n",
    "                              skew(fd_mahalD_H, axis=1, bias=True)))        \n",
    "    \n",
    "    adc = np.abs(adc_mahal_P-adc_mahal_H)\n",
    "    print(adc.shape)\n",
    "    fa = np.abs(fa_mahal_P-fa_mahal_H)\n",
    "    fd = np.abs(fd_mahal_P-fd_mahal_H)\n",
    "    feature_all = np.vstack((fa,adc,fd))\n",
    "    \n",
    "    return feature_all, adc_mahalD_P, adc_mahalD_H,fa_mahalD_P, fa_mahalD_H, fd_mahalD_P, fd_mahalD_H \n",
    "\n",
    "def cal_mahal_profile(mahalD,data,Nsub):\n",
    "    # mahalD: all the strms distance from distribution shape = 65*5000\n",
    "    # data: all strms measures shape = 5000*100*65\n",
    "    data_wighted_strm = np.zeros(shape = (100,Nsub))\n",
    "    for i in range(Nsub):\n",
    "        #test = np.argsort(mahalD)\n",
    "        data_wighted_strm[:,:,i] = ((1/mahalD[i,:].reshape(-1,1))*data[:,:,i])/(1/mahalD[i,:]).sum()\n",
    "        #data_wighted_strm[:,i] = ((1/mahalD[i,:].reshape(-1,1))*data[:,:,i]).sum(axis=0)\n",
    "    return data_wighted_strm.T\n",
    "\n",
    "def length_tract():\n",
    "    paths = sorted(glob.glob('/Users/boshra/Desktop/Boshra/length/*.txt'), key=lambda x:float(re.findall(\"(\\d+)\",x)[0]))\n",
    "    list_of_dfs = [pd.read_csv(path, skip_blank_lines=True, header = None, na_values = ['no info', ';']) for path in paths] \n",
    "    length = np.zeros(shape=(130,5000))\n",
    "    for j in range(0, len(list_of_dfs),1):  \n",
    "        df = list_of_dfs[j]    \n",
    "        length[j] = df[0][:]\n",
    "    return length\n",
    "\n",
    "def extract_value_H_P(adc_data, fa_data, fd_data, rd_data, ad_data, PathSide, subjN):\n",
    "    \n",
    "    adc_data_P = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    fa_data_P = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    fd_data_P = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    rd_data_P = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    ad_data_P = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "\n",
    "    \n",
    "    adc_data_H = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    fa_data_H = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    fd_data_H = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    rd_data_H = np.zeros(shape=(5000,100,subjN), dtype = np.float64)\n",
    "    ad_data_H = np.zeros(shape=(5000,100,subjN), dtype = np.float64)    \n",
    "    pl = 0\n",
    "    pr = 0\n",
    "    for n in range(0,subjN):  \n",
    "        # Pathology placed at left hemisphere\n",
    "\n",
    "        if (PathSide[n]==0):\n",
    "            adc_data_P[:,:,n] = adc_data[:,:,n*2]  \n",
    "            #adc_data_PL[pl] = adc_data[:,:,n*2]\n",
    "            fa_data_P[:,:,n] = fa_data[:,:,n*2]\n",
    "            fd_data_P[:,:,n] = fd_data[:,:,n*2]\n",
    "            ad_data_P[:,:,n] = ad_data[:,:,n*2]\n",
    "            rd_data_P[:,:,n] = rd_data[:,:,n*2]\n",
    "            #length_P[n,:] = length[n*2,:]\n",
    "\n",
    "            adc_data_H[:,:,n] = adc_data[:,:,n*2+1]  \n",
    "            fa_data_H[:,:,n] = fa_data[:,:,n*2+1]\n",
    "            fd_data_H[:,:,n] = fd_data[:,:,n*2+1]\n",
    "            ad_data_H[:,:,n] = ad_data[:,:,n*2+1]\n",
    "            rd_data_H[:,:,n] = rd_data[:,:,n*2+1]            \n",
    "            #length_H[n,:]= length[n*2+1,:]\n",
    "            \n",
    "            pl = pl +1\n",
    "        # Pathology placed at right hemisphere    \n",
    "        else:\n",
    "            adc_data_P[:,:,n] = adc_data[:,:,n*2+1]\n",
    "            fa_data_P[:,:,n] = fa_data[:,:,n*2+1]\n",
    "            fd_data_P[:,:,n] = fd_data[:,:,n*2+1]\n",
    "            ad_data_P[:,:,n] = ad_data[:,:,n*2+1]\n",
    "            rd_data_P[:,:,n] = rd_data[:,:,n*2+1]\n",
    "            #length_P[n,:] = length[n*2+1,:]\n",
    "            \n",
    "            adc_data_H[:,:,n] = adc_data[:,:,n*2]  \n",
    "            fa_data_H[:,:,n] = fa_data[:,:,n*2]\n",
    "            fd_data_H[:,:,n] = fd_data[:,:,n*2]\n",
    "            ad_data_H[:,:,n] = ad_data[:,:,n*2]\n",
    "            rd_data_H[:,:,n] = rd_data[:,:,n*2]\n",
    "            #length_H[n,:] = length[n*2,:]\n",
    "            \n",
    "            pr = pr +1\n",
    "        \n",
    "    return adc_data_P,fa_data_P,fd_data_P, ad_data_P, rd_data_P, adc_data_H,fa_data_H,fd_data_H, ad_data_H, rd_data_H\n",
    "\n",
    "def sub_divide_hemispheres(data,PathSide, subjN):\n",
    "    P_at_right = PathSide.sum()\n",
    "    P_at_left = subjN - P_at_right\n",
    "    left = data[:,:,::2]\n",
    "    right = data[:,:,1:][:,:,::2]\n",
    "    sub_data_left = dict()\n",
    "    sub_data_right = dict()\n",
    "    sub_data_left['H'] = np.zeros(shape = (5000,100,P_at_right), dtype = np.float64)\n",
    "    sub_data_left['P'] = np.zeros(shape = (5000,100,P_at_left), dtype = np.float64)\n",
    "    sub_data_right['H'] = np.zeros(shape = (5000,100,P_at_left), dtype = np.float64)\n",
    "    sub_data_right['P'] = np.zeros(shape = (5000,100,P_at_right), dtype = np.float64)\n",
    "    l = 0\n",
    "    r = 0\n",
    "    for n in range(0,subjN):  \n",
    "        # Pathology placed at left hemisphere l=0\n",
    "\n",
    "        if (PathSide[n]==0):\n",
    "            sub_data_right['H'][:,:,l] = right[:,:,n]\n",
    "            sub_data_left['P'][:,:,l] = left[:,:,n]\n",
    "            l=l+1\n",
    "            \n",
    "        else:\n",
    "            sub_data_left['H'][:,:,r] = left[:,:,n]\n",
    "            sub_data_right['P'][:,:,r] = right[:,:,n]\n",
    "            r=r+1      \n",
    "    \n",
    "    return sub_data_left, sub_data_right\n",
    "\n",
    "def loadrawdata_R_new(strr, subN):\n",
    "    paths = sorted(glob.glob('/Users/boshra/Desktop/Boshra/all_patients/'+strr+'/*.csv'), key=lambda x:float(re.findall(\"(\\d+)\",x)[1]))   \n",
    "    for i in range(subN):\n",
    "        paths[i*2:i*2+2] = sorted(paths[i*2:i*2+2])\n",
    "    list_of_dfs = [pd.read_csv(path, skip_blank_lines=True, header = None, na_values = ['no info', ';']) for path in paths]\n",
    "        \n",
    "    # DATA_LR = {}\n",
    "    DATA = np.empty(shape = (5000,100,subN*2), dtype = np.float64)\n",
    "    DATA[:,:,:] = np.NaN\n",
    "    for j in range(0,subN*2):  \n",
    "        #print(j)\n",
    "        df = list_of_dfs[j]\n",
    "        #df = df.replace(0,nan)\n",
    "        df = df.reset_index(drop=True)        \n",
    "        for i in range(1,len(df[0])):            \n",
    "            listt = [x for x in df[0][i].split(' ') if x]\n",
    "            tt = np.asarray(listt, dtype = np.float32)\n",
    "            # DATA[i-1,0:len(tt),j] = tt # command before resampling\n",
    "            DATA[i-1,:,j] = tt\n",
    "            # Save the length of streamlines in index 203 (the last item)\n",
    "            # DATA[i-1,203,j] = len(tt)  # command before resampling\n",
    "    return DATA\n",
    "def check_data(data):\n",
    "    for i in range(data.shape[2]):\n",
    "        print(i,':',len(np.argwhere(np.isnan(data[:,:,i]))))\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
